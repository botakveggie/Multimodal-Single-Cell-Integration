{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x192377a0b90>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO INPUT DIMENSION DEFINED\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "VERBOSE = 1\n",
    "INPUT_DIM = 0\n",
    "if INPUT_DIM==0:\n",
    "    print('NO INPUT DIMENSION DEFINED')\n",
    "# HIDDEN_DIM = 200\n",
    "LEARNING_RATE = .01\n",
    "BATCH_SIZE = 400\n",
    "NUM_EPOCHS = 200\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes and functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A pytorch dataset class that accepts an inputs path, and optionally a targets path. \n",
    "    This class holds all the data and implements a __getitem__ method to be used by a \n",
    "    Python generator object or other classes that need it.\n",
    "\n",
    "    Properties:\n",
    "        pd_csv: bool, default=False\n",
    "            Whether the input data is a csv file\n",
    "        inputs: h5py.File or pandas.DataFrame\n",
    "            Either a h5py.File or pd.DataFrame object containing the inputs.\n",
    "        targets: h5py.File or pandas.DataFrame\n",
    "            Either a h5py.File or pd.DataFrame object containing the targets.\n",
    "        num_cells: int\n",
    "            Number of rows in the Dataset.\n",
    "        num_features: int or None\n",
    "            Number of features in the input data.\n",
    "        num_cells: int or None\n",
    "            Number of columns in the target.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs_path: str, targets_path: str or None = None):\n",
    "        \"\"\"\n",
    "        Read the content of the inputs file\n",
    "        Args:\n",
    "            inputs_path: string\n",
    "                Path to the inputs file.\n",
    "            targets_path: string, default=None\n",
    "                Path to the targets file.\n",
    "        \"\"\"\n",
    "        file_extension = inputs_path.strip().split('.')[-1]\n",
    "        self.pd_csv = (file_extension ==\"csv\")\n",
    "        if self.pd_csv: \n",
    "            # Initialises CiteDataset from pandas csv\n",
    "\n",
    "            # Store the values into self.inputs\n",
    "            self.inputs = pd.read_csv(inputs_path, index_col=0)\n",
    "\n",
    "            # Values that inform the shape of the input matrix\n",
    "            self.num_cells = self.inputs.shape[0]\n",
    "            self.num_features = self.inputs.shape[1]\n",
    "            if targets_path: # Init CiteDataset for training\n",
    "                self.targets = pd.read_csv(targets_path, index_col=0)\n",
    "\n",
    "                # Values that inform the shape of the targets matrix\n",
    "                self.num_targets = self.targets.shape[1]\n",
    "        self.protein_ids = list(self.targets.columns)\n",
    "        self.cell_ids = list(self.targets.index)\n",
    "\n",
    "    def data_size(self):\n",
    "        \"\"\"\n",
    "        A function to inform the dimensions of the data. The function returns \n",
    "        a tuple of two integers:\n",
    "            num_features: int\n",
    "                Number of features in the input data.\n",
    "            num_targets: int\n",
    "                Number of target outputs.\n",
    "        \"\"\"\n",
    "        return self.num_features, self.num_targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of instances in the data\n",
    "        \"\"\"\n",
    "        return self.num_cells\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Return the i-th instance in the format of:\n",
    "            (inputs, targets), where inputs and targets are PyTorch tensors.\n",
    "        \"\"\"\n",
    "        input_row = self.inputs.iloc[i,:]\n",
    "        inputs = torch.tensor(input_row) \n",
    "        targets = None\n",
    "        if hasattr(self, 'targets'):\n",
    "            targets = torch.tensor(self.targets.iloc[i,:])\n",
    "        \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCBlock(nn.Module):\n",
    "    \"A single feed-forward model with num_features input nodes and  num_hidden output nodes, with a specified dropout rate.\"\n",
    "    def __init__(self, num_features: int, num_hidden: int, dropout: float):\n",
    "        super(FCBlock, self).__init__()\n",
    "        self.Input_Layer = nn.Linear(num_features, num_hidden)\n",
    "        self.Dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Performs Components in sequence\"\n",
    "        x = self.Input_Layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Dropout(x)\n",
    "        print(\"forward from FC\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module to generate embeddings of a RNA vector\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features: int):\n",
    "        super().__init__()\n",
    "        self.l0 = FCBlock(num_features, 120, 0.05)\n",
    "        self.l1 = FCBlock(120, 60, 0.05)\n",
    "        self.l2 = FCBlock(60, 30, 0.05)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l0(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        print(\"forward from Enc\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module to extract Protein sequences from RNA embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, num_targets: int):\n",
    "        super().__init__()\n",
    "        self.l0 = FCBlock(30, 70, 0.05)\n",
    "        self.l1 = FCBlock(70, 100, 0.05)\n",
    "        self.l2 = FCBlock(100, num_targets, 0.05)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l0(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        print(\"forward from Dec\")\n",
    "        return x\n",
    "    \n",
    "class CiteseqModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for the Encoder and Decoder modules\n",
    "    Converts RNA sequence to Protein sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features: int, num_targets: int):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_features)\n",
    "        self.decoder = Decoder(num_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.encoder(x)\n",
    "        outputs = self.decoder(embeddings)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, labels):\n",
    "    \"\"\" MSE Loss function\"\"\"\n",
    "    return nn.MSELoss()(outputs, labels)\n",
    "\n",
    "def correlation_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Scores the predictions according to the competition rules. \n",
    "    It is assumed that the predictions are not constant.\n",
    "    Returns the average of each sample's Pearson correlation coefficient\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(y_true) == pd.DataFrame: y_true = y_true.values\n",
    "    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)\n",
    "\n",
    "def collator(batch):\n",
    "    \"\"\"\n",
    "    Define a function that receives a list of (inputs, targets) pair for each cell\n",
    "    and return a pair of tensors:\n",
    "        batch_inputs: a tensor that combines all the inputs in the mini-batch\n",
    "        batch_targets: a tensor that combines all the targets in the mini-batch\n",
    "    \"\"\"\n",
    "    # batch_inputs tensor dimensions: batch_dim x INPUT_DIM\n",
    "    # batch_targets tensor dimensions: batch_dim x num_targets\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    batch_inputs = torch.cat([tens.unsqueeze(0) for tens in inputs])\n",
    "    if targets: # for training, return both texts and batch_targets\n",
    "        batch_targets = torch.cat([tens.unsqueeze(0) for tens in targets])\n",
    "    return batch_inputs, batch_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, learning_rate, num_epoch, device='cpu', model_path=None, loss_fn=nn.MSELoss, optim = optim.Adam):\n",
    "    \"\"\"\n",
    "    Complete the training procedure below by specifying the loss function\n",
    "    and optimizers with the specified learning rate and specified number of epoch.\n",
    "    \"\"\"\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
    "\n",
    "    # assign these variables\n",
    "    criterion=loss_fn()\n",
    "    optimizer = optim(model.parameters(), lr=learning_rate)\n",
    "    # scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "    # print('no model params:', ([p.shape for p in list(model.parameters())]))\n",
    "\n",
    "    # train, validation split?\n",
    "    # do here\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for step, data in enumerate(data_loader):\n",
    "            # get the inputs; data is a tuple of (inputs_tensor, targets_tensor)\n",
    "            inputs = data[0].to(device)\n",
    "            targets = data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # do forward propagation\n",
    "            y_preds = model(inputs.float())\n",
    "\n",
    "            # do loss calculation\n",
    "            # dont count loss for padding\n",
    "            if VERBOSE == 4: print('    CHECKING LOSS...')\n",
    "            loss_tensor = criterion(input= y_preds, target= targets)\n",
    "            if VERBOSE == 1:\n",
    "                print(loss_tensor)\n",
    "                input('^LOSS')\n",
    "            # loss_tensor = ((y_preds-labels)**2).sum()\n",
    "\n",
    "            # if step%50==1: \n",
    "                # print('epoch',epoch,'step',step,'Loss:', loss_tensor)\n",
    "\n",
    "            # do backward propagation\n",
    "            loss_tensor.backward()\n",
    "            # do parameter optimization step\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate running loss value\n",
    "            running_loss += loss_tensor.item()\n",
    "            # print('running loss updated to', running_loss)\n",
    "\n",
    "            # print loss value every 100 steps and reset the running loss\n",
    "                # input()\n",
    "        # scheduler.step()\n",
    "        if VERBOSE == 1:\n",
    "            print('[Epoch %d, Step %5d] MSE loss: %.3f' %\n",
    "                (epoch + 1, step + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "        # Check correlation score with validation set\n",
    "        # correlation_score()\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    # define the checkpoint and save it to the model path\n",
    "    # tip: the checkpoint can contain more than just the model\n",
    "    checkpoint = {\n",
    "        'epoch':epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'vocab':\n",
    "            {'labels index': dataset.labels_index,\n",
    "            'bigram index': dataset.bigram_index,}\n",
    "        # 'loss':running_loss\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "\n",
    "    print('Model saved in ', model_path)\n",
    "    print('Training finished in {} minutes.'.format((end - start).seconds / 60.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the inputs and targets path\n",
    "inputs_path = 'C:/Users/xiaox/Documents/GitHub/Multimodal-Single-Cell-Integration/data/train_cite_inputs_reduced.csv'\n",
    "targets_path = 'C:/Users/xiaox/Documents/GitHub/Multimodal-Single-Cell-Integration/data/train_cite_targets_reduced.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO INPUT DIMENSION DEFINED\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = 1\n",
    "INPUT_DIM = 0\n",
    "if INPUT_DIM==0:\n",
    "    print('NO INPUT DIMENSION DEFINED')\n",
    "# HIDDEN_DIM = 200\n",
    "LEARNING_RATE = .01\n",
    "BATCH_SIZE = 400\n",
    "NUM_EPOCHS = 200\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward from FC\n",
      "forward from FC\n",
      "forward from FC\n",
      "forward from Enc\n",
      "forward from FC\n",
      "forward from FC\n",
      "forward from FC\n",
      "forward from Dec\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MSELoss.__init__() got an unexpected keyword argument 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [86], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m num_features, num_targets \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mdata_size()\n\u001b[0;32m      4\u001b[0m basemodel \u001b[39m=\u001b[39m CiteseqModel(num_features, num_targets)\n\u001b[1;32m----> 6\u001b[0m train(model\u001b[39m=\u001b[39mbasemodel, batch_size\u001b[39m=\u001b[39m\u001b[39m400\u001b[39m, dataset\u001b[39m=\u001b[39mdataset, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m, num_epoch\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, model_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel/basemodel\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [83], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataset, batch_size, learning_rate, num_epoch, device, model_path, loss_fn, optim)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# do loss calculation\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# dont count loss for padding\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m VERBOSE \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m: \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m    CHECKING LOSS...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m loss_tensor \u001b[39m=\u001b[39m criterion(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m y_preds, target\u001b[39m=\u001b[39;49m targets)\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m VERBOSE \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     38\u001b[0m     \u001b[39mprint\u001b[39m(loss_tensor)\n",
      "\u001b[1;31mTypeError\u001b[0m: MSELoss.__init__() got an unexpected keyword argument 'input'"
     ]
    }
   ],
   "source": [
    "dataset = CiteDataset(inputs_path, targets_path)\n",
    "num_features, num_targets = dataset.data_size()\n",
    "\n",
    "basemodel = CiteseqModel(num_features, num_targets)\n",
    "\n",
    "train(model=basemodel, batch_size=400, dataset=dataset, learning_rate=0.05, num_epoch=10, model_path='model/basemodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CiteseqModel.__init__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset \u001b[39m=\u001b[39m CiteDataset(inputs_path, targets_path)\n\u001b[0;32m      2\u001b[0m num_features, num_targets \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mdata_size()\n\u001b[1;32m----> 3\u001b[0m basemodel \u001b[39m=\u001b[39m CiteseqModel(num_features, num_targets, DROPOUT)\n\u001b[0;32m      4\u001b[0m train(basemodel, dataset, BATCH_SIZE ,LEARNING_RATE, NUM_EPOCHS, device_str, \u001b[39m'\u001b[39m\u001b[39mmodel/basemodel\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: CiteseqModel.__init__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "dataset = CiteDataset(inputs_path, targets_path)\n",
    "num_features, num_targets = dataset.data_size()\n",
    "basemodel = CiteseqModel(num_features, num_targets, DROPOUT)\n",
    "train(basemodel, dataset, BATCH_SIZE ,LEARNING_RATE, NUM_EPOCHS, device_str, 'model/basemodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset: CiteDataset, device='cpu'):\n",
    "    \"\"\"\n",
    "    Function to test model on the test dataset. \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(dataset, batch_size=20, collate_fn=collator, shuffle=False)\n",
    "    preds_tensor = None\n",
    "    with torch.no_grad():\n",
    "        for _,data in enumerate(data_loader):\n",
    "            targets = data[0].to(device)\n",
    "            # print((texts.shape))\n",
    "            outputs = model(targets).cpu()\n",
    "            if preds_tensor is None:\n",
    "                preds_tensor = outputs\n",
    "            else:\n",
    "                torch.cat((preds_tensor, outputs), dim=0)\n",
    "            # get the label predictions\n",
    "    preds = pd.DataFrame(preds_tensor, columns=dataset.protein_ids, index=dataset.cell_ids)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = test(basemodel, dataset)\n",
    "preds.to_csv('output/base_preds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
