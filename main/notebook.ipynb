{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12a1110d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO INPUT DIMENSION DEFINED\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "VERBOSE = 1\n",
    "INPUT_DIM = 0\n",
    "if INPUT_DIM==0:\n",
    "    print('NO INPUT DIMENSION DEFINED')\n",
    "# HIDDEN_DIM = 200\n",
    "LEARNING_RATE = .01\n",
    "BATCH_SIZE = 400\n",
    "NUM_EPOCHS = 200\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes and functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CiteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A pytorch dataset class that accepts an inputs path, and optionally a targets path. \n",
    "    This class holds all the data and implements a __getitem__ method to be used by a \n",
    "    Python generator object or other classes that need it.\n",
    "\n",
    "    Properties:\n",
    "        pd_csv: bool, default=False\n",
    "            Whether the input data is a csv file\n",
    "        inputs: h5py.File or pandas.DataFrame\n",
    "            Either a h5py.File or pd.DataFrame object containing the inputs.\n",
    "        targets: h5py.File or pandas.DataFrame\n",
    "            Either a h5py.File or pd.DataFrame object containing the targets.\n",
    "        num_cells: int\n",
    "            Number of rows in the Dataset.\n",
    "        num_features: int or None\n",
    "            Number of features in the input data.\n",
    "        num_cells: int or None\n",
    "            Number of columns in the target.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs_path: str, targets_path: str or None = None):\n",
    "        \"\"\"\n",
    "        Read the content of the inputs file\n",
    "        Args:\n",
    "            inputs_path: string\n",
    "                Path to the inputs file.\n",
    "            targets_path: string, default=None\n",
    "                Path to the targets file.\n",
    "        \"\"\"\n",
    "        file_extension = inputs_path.strip().split('.')[-1]\n",
    "        self.pd_csv = (file_extension ==\"csv\")\n",
    "        if self.pd_csv: \n",
    "            # Initialises CiteDataset from pandas csv\n",
    "\n",
    "            # Store the values into self.inputs\n",
    "            self.inputs = pd.read_csv(inputs_path, index_col=0)\n",
    "\n",
    "            # Values that inform the shape of the input matrix\n",
    "            self.num_cells = self.inputs.shape[0]\n",
    "            self.num_features = self.inputs.shape[1]\n",
    "            if targets_path: # Init CiteDataset for training\n",
    "                self.targets = pd.read_csv(targets_path, index_col=0)\n",
    "\n",
    "                # Values that inform the shape of the targets matrix\n",
    "                self.num_targets = self.targets.shape[1]\n",
    "        self.protein_ids = list(self.targets.columns)\n",
    "        self.cell_ids = list(self.targets.index)\n",
    "\n",
    "    def data_size(self):\n",
    "        \"\"\"\n",
    "        A function to inform the dimensions of the data. The function returns \n",
    "        a tuple of two integers:\n",
    "            num_features: int\n",
    "                Number of features in the input data.\n",
    "            num_targets: int\n",
    "                Number of target outputs.\n",
    "        \"\"\"\n",
    "        return self.num_features, self.num_targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of instances in the data\n",
    "        \"\"\"\n",
    "        return self.num_cells\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Return the i-th instance in the format of:\n",
    "            (inputs, targets), where inputs and targets are PyTorch tensors.\n",
    "        \"\"\"\n",
    "        input_row = self.inputs.iloc[i,:]\n",
    "        inputs = torch.tensor(input_row) \n",
    "        targets = None\n",
    "        if hasattr(self, 'targets'):\n",
    "            targets = torch.tensor(self.targets.iloc[i,:])\n",
    "        \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCBlock(nn.Module):\n",
    "    \"A single feed-forward model with num_features input nodes and  num_hidden output nodes, with a specified dropout rate.\"\n",
    "    def __init__(self, num_features: int, num_hidden: int, dropout: float):\n",
    "        super(FCBlock, self).__init__()\n",
    "        self.Input_Layer = nn.Linear(num_features, num_hidden)\n",
    "        self.Dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Performs Components in sequence\"\n",
    "        x = self.Input_Layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module to generate embeddings of a RNA vector\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features: int):\n",
    "        super().__init__()\n",
    "        self.l0 = FCBlock(num_features, 120, 0.05)\n",
    "        self.l1 = FCBlock(120, 60, 0.05)\n",
    "        self.l2 = FCBlock(60, 30, 0.05)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l0(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module to extract Protein sequences from RNA embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, num_targets: int):\n",
    "        super().__init__()\n",
    "        self.l0 = FCBlock(30, 70, 0.05)\n",
    "        self.l1 = FCBlock(70, 100, 0.05)\n",
    "        self.l2 = FCBlock(100, num_targets, 0.05)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l0(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "    \n",
    "class CiteseqModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for the Encoder and Decoder modules\n",
    "    Converts RNA sequence to Protein sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features: int, num_targets: int):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_features)\n",
    "        self.decoder = Decoder(num_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.encoder(x)\n",
    "        outputs = self.decoder(embeddings)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, labels):\n",
    "    \"\"\" MSE Loss function\"\"\"\n",
    "    return nn.MSELoss()(outputs, labels)\n",
    "\n",
    "def correlation_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Scores the predictions according to the competition rules. \n",
    "    It is assumed that the predictions are not constant.\n",
    "    Returns the average of each sample's Pearson correlation coefficient\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(y_true) == pd.DataFrame: y_true = y_true.values\n",
    "    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)\n",
    "\n",
    "def collator(batch):\n",
    "    \"\"\"\n",
    "    Define a function that receives a list of (inputs, targets) pair for each cell\n",
    "    and return a pair of tensors:\n",
    "        batch_inputs: a tensor that combines all the inputs in the mini-batch\n",
    "        batch_targets: a tensor that combines all the targets in the mini-batch\n",
    "    \"\"\"\n",
    "    # batch_inputs tensor dimensions: batch_dim x INPUT_DIM\n",
    "    # batch_targets tensor dimensions: batch_dim x num_targets\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    batch_inputs = torch.cat(inputs)\n",
    "    if targets: # for training, return both texts and batch_targets\n",
    "        batch_targets = torch.cat(targets)\n",
    "    return batch_inputs, batch_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, learning_rate, num_epoch, device='cpu', model_path=None, loss_fn=nn.MSELoss, optim = optim.Adam):\n",
    "    \"\"\"\n",
    "    Complete the training procedure below by specifying the loss function\n",
    "    and optimizers with the specified learning rate and specified number of epoch.\n",
    "    \"\"\"\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
    "\n",
    "    # assign these variables\n",
    "    criterion=loss_fn\n",
    "    optimizer = optim(model.parameters(), lr=learning_rate)\n",
    "    # scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "    # print('no model params:', ([p.shape for p in list(model.parameters())]))\n",
    "\n",
    "    # train, validation split?\n",
    "    # do here\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for step, data in enumerate(data_loader):\n",
    "            # get the inputs; data is a tuple of (inputs_tensor, targets_tensor)\n",
    "            inputs = data[0].to(device)\n",
    "            targets = data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # do forward propagation\n",
    "            y_preds = model(inputs)\n",
    "\n",
    "            # do loss calculation\n",
    "            # dont count loss for padding\n",
    "            if VERBOSE == 4: print('    CHECKING LOSS...')\n",
    "            loss_tensor = criterion(input= y_preds, target= targets)\n",
    "            if VERBOSE == 1:\n",
    "                print(loss_tensor)\n",
    "                input('^LOSS')\n",
    "            # loss_tensor = ((y_preds-labels)**2).sum()\n",
    "\n",
    "            # if step%50==1: \n",
    "                # print('epoch',epoch,'step',step,'Loss:', loss_tensor)\n",
    "\n",
    "            # do backward propagation\n",
    "            loss_tensor.backward()\n",
    "            # do parameter optimization step\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate running loss value\n",
    "            running_loss += loss_tensor.item()\n",
    "            # print('running loss updated to', running_loss)\n",
    "\n",
    "            # print loss value every 100 steps and reset the running loss\n",
    "                # input()\n",
    "        # scheduler.step()\n",
    "        if VERBOSE == 1:\n",
    "            print('[Epoch %d, Step %5d] MSE loss: %.3f' %\n",
    "                (epoch + 1, step + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "        # Check correlation score with validation set\n",
    "        # correlation_score()\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    # define the checkpoint and save it to the model path\n",
    "    # tip: the checkpoint can contain more than just the model\n",
    "    checkpoint = {\n",
    "        'epoch':epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'vocab':\n",
    "            {'labels index': dataset.labels_index,\n",
    "            'bigram index': dataset.bigram_index,}\n",
    "        # 'loss':running_loss\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "\n",
    "    print('Model saved in ', model_path)\n",
    "    print('Training finished in {} minutes.'.format((end - start).seconds / 60.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the inputs and targets path\n",
    "inputs_path = '.'\n",
    "targets_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CiteDataset(inputs_path, targets_path)\n",
    "num_features, num_targets = dataset.data_size\n",
    "\n",
    "basemodel = CiteseqModel(num_features, num_targets)\n",
    "\n",
    "train(model=basemodel, dataset=dataset, learning_rate=0.05, num_epoch=10, model_path='model/basemodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset: CiteDataset, device='cpu'):\n",
    "    \"\"\"\n",
    "    Function to test model on the test dataset. \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(dataset, batch_size=20, collate_fn=collator, shuffle=False)\n",
    "    preds_tensor = None\n",
    "    with torch.no_grad():\n",
    "        for _,data in enumerate(data_loader):\n",
    "            targets = data[0].to(device)\n",
    "            # print((texts.shape))\n",
    "            outputs = model(targets).cpu()\n",
    "            if preds_tensor is None:\n",
    "                preds_tensor = outputs\n",
    "            else:\n",
    "                torch.cat((preds_tensor, outputs), dim=0)\n",
    "            # get the label predictions\n",
    "    preds = pd.DataFrame(preds_tensor, columns=dataset.protein_ids, index=dataset.cell_ids)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = test(basemodel, dataset)\n",
    "preds.to_csv('output/base_preds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
